{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q__UuuazrxUI",
        "outputId": "a585515e-f1dd-425f-cd1b-45b845c5f420",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GjD2DCDrrczE"
      },
      "outputs": [],
      "source": [
        "import sys, torch, os, tarfile, pickle\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import multiprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "K4wvwpLyrtBG"
      },
      "outputs": [],
      "source": [
        "sys.path.append('/content/drive/MyDrive')\n",
        "import utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2HrdcXariY9"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jbe_6wwiqyER"
      },
      "outputs": [],
      "source": [
        "def train_model_on_fold(train_set, val_set, device, parameters, num_epochs):\n",
        "  #Define data loaders\n",
        "  train_fold_loader = DataLoader(train_set, batch_size=1, shuffle=True)\n",
        "  val_fold_loader = DataLoader(val_set, batch_size=1)\n",
        "\n",
        "  #Define model\n",
        "  model = utils.RNA_Unet(channels = parameters[\"conv2_filters\"])\n",
        "  model.to(device)\n",
        "  optimizer = utils.adam_optimizer(model, parameters[\"lr\"], parameters[\"weight_decay\"])\n",
        "\n",
        "  #Train model\n",
        "  for epoch in range(num_epochs):\n",
        "      for input, output, _ in train_fold_loader:\n",
        "          input, output = input.to(device), output.to(device)\n",
        "          output = output.unsqueeze(1)\n",
        "          optimizer.zero_grad()\n",
        "          predicted = model(input)\n",
        "          loss = utils.dice_loss(predicted, output)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      #Evaluate model on validation set\n",
        "      val_loss = 0.0\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for input, output, _ in val_fold_loader:\n",
        "              input, output = input.to(device), output.to(device)\n",
        "              output = output.unsqueeze(1)\n",
        "              predicted = model(input)\n",
        "              val_loss += (utils.dice_loss(predicted, output)).item()\n",
        "      val_loss = val_loss/len(val_fold_loader)\n",
        "      return val_loss\n",
        "\n",
        "def Kfold_cv(parameters: dict, device, train_set, num_epochs, k=5):\n",
        "    val_losses = 0.0\n",
        "\n",
        "    #Split data into k folds:\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "    # Prepare arguments for parallel processing\n",
        "    args = [(torch.utils.data.Subset(train_set, train_idx), torch.utils.data.Subset(train_set, val_idx), device, parameters, num_epochs)\n",
        "            for train_idx, val_idx in kf.split(train_set)]\n",
        "\n",
        "    # Set start method to 'spawn' for multiprocessing\n",
        "    multiprocessing.set_start_method('spawn', force=True)\n",
        "\n",
        "    #Parallel processing of folds\n",
        "    with multiprocessing.Pool() as pool:\n",
        "      val_losses = pool.starmap(train_model_on_fold, args)\n",
        "\n",
        "    #Return average validation loss\n",
        "    return sum(val_losses)/k\n",
        "\n",
        "def adaptive_hyperparameter_search(train_set, num_epochs, lr_range, weight_decay_range, conv2_filters_range, use_cuda = True, trials = 10, k = 5):\n",
        "  \"\"\"\n",
        "  Function that performs adaptive hyperparameter search for RNAUnet using CuPy (if cuda available)\n",
        "\n",
        "  Args:\n",
        "  - train_set: Pytorch training data set\n",
        "  - val_set: Pytorch validation data set\n",
        "  - num_epochs: Maximum number of epochs to train the model\n",
        "  - lr_range: Range og learning rate to search\n",
        "  - weight_decay_range: Range of weight decay values to search\n",
        "  - conv2_filters_range: Range of numbers of filters for the first hidden layer to search\n",
        "  - use_cuda: Boolean value indicating whether to use cuda if available\n",
        "  - trials: Number of trials to perform\n",
        "  - k: Number of folds for cross-validation\n",
        "\n",
        "  Returns:\n",
        "  - best_params: Dictionary containing the est hyperparameters found.\n",
        "  \"\"\"\n",
        "\n",
        "  if use_cuda and torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "  best_loss = float('inf')\n",
        "  best_params ={}\n",
        "\n",
        "  #Define search space\n",
        "  params = {\n",
        "        \"lr\": lr_range,\n",
        "        \"weight_decay\": weight_decay_range,\n",
        "        \"conv2_filters\": conv2_filters_range\n",
        "    }\n",
        "\n",
        "\n",
        "  for i in range(trials):\n",
        "    #Get search space for this iteration\n",
        "    parameters = {'lr': np.random.choice(params[\"lr\"]),\n",
        "                  'weight_decay': np.random.choice(params[\"weight_decay\"]),\n",
        "                  'conv2_filters': np.random.choice(params[\"conv2_filters\"])}\n",
        "\n",
        "    print(f\"Start trial number {i+1} with parameters: {parameters}\")\n",
        "\n",
        "    val_loss = Kfold_cv(parameters, device, train_set, num_epochs, k)\n",
        "\n",
        "    #Update best hyperparameters if applicable\n",
        "    if val_loss < best_loss:\n",
        "      best_loss = val_loss\n",
        "      best_params = parameters\n",
        "      print(\"New best hyperparameters found: \")\n",
        "      print(best_params)\n",
        "\n",
        "    #Update search space based on performance\n",
        "    if i > 0:\n",
        "      if val_loss < prev_val_loss:\n",
        "        if parameters[\"lr\"] in params[\"lr\"]:\n",
        "          params[\"lr\"].remove(parameters[\"lr\"])\n",
        "        if parameters[\"weight_decay\"] in params[\"weight_decay\"]:\n",
        "          params[\"weight_decay\"].remove(parameters[\"weight_decay\"])\n",
        "        if parameters[\"conv2_filters\"] in params[\"conv2_filters\"]:\n",
        "          params[\"conv2_filters\"].remove(parameters[\"conv2_filters\"])\n",
        "\n",
        "        prev_val_loss = val_loss\n",
        "\n",
        "    print(f\"Trial {i+1} completed. Best hyperparameters found: {best_params} with loss {best_loss}\")\n",
        "\n",
        "  return best_params\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA-zsKMWriiX"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zv0Vur_voeMU"
      },
      "outputs": [],
      "source": [
        "RNA_data = namedtuple('RNA_data', 'input output length family name pairs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4WNg06ZQB3ol"
      },
      "outputs": [],
      "source": [
        "# Define the path to the zipped folder in your Google Drive\n",
        "tar_file_path = '/content/drive/MyDrive/data/experiment8.tar.gz'\n",
        "\n",
        "\n",
        "# Extract the tar.gz archive\n",
        "with tarfile.open(tar_file_path, 'r:gz') as tar:\n",
        "    tar.extractall('/content')\n",
        "\n",
        "file_list = [os.path.join('data', 'experiment8', file) for file in os.listdir('data/experiment8')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KxvGx-pdlCCp"
      },
      "outputs": [],
      "source": [
        "train = pickle.load(open('/content/drive/MyDrive/data/experiment_train.pkl', 'rb'))\n",
        "valid = pickle.load(open('/content/drive/MyDrive/data/experiment_valid.pkl', 'rb'))\n",
        "\n",
        "family_map = pickle.load(open('/content/drive/MyDrive/data/experiment_familymap.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "abCg6CrjpzB8"
      },
      "outputs": [],
      "source": [
        "# Define your train_dataset and validation_dataset\n",
        "train_dataset = utils.ImageToImageDataset(train, family_map)\n",
        "validation_dataset = utils.ImageToImageDataset(valid, family_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJyYjrt9rix7"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "86YWyt4VzymX"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "        \"lr\": [0.01, 0.005, 0.001],\n",
        "        \"weight_decay\": [0.01, 0.001, 0.0001, 0],\n",
        "        \"conv2_filters\": [32, 64],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn0vh7UTzymX"
      },
      "outputs": [],
      "source": [
        "adaptive_hyperparameter_search(train_dataset, 10, params[\"lr\"], params[\"weight_decay\"], params[\"conv2_filters\"], use_cuda = True, trials = 10, k = 5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}