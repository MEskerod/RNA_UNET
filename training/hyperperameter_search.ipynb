{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q__UuuazrxUI"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GjD2DCDrrczE"
      },
      "outputs": [],
      "source": [
        "import sys, torch, os, tarfile, pickle\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils import Dataloader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QDfYjKo8n_Jr"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import cupy as cp\n",
        "except ImportError:\n",
        "    cp = np  # If CuPy is not available, fallback to NumPy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4wvwpLyrtBG"
      },
      "outputs": [],
      "source": [
        "sys.path.append('/content/drive/MyDrive')\n",
        "import utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2HrdcXariY9"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbe_6wwiqyER"
      },
      "outputs": [],
      "source": [
        "def adaptive_hyperparameter_search(train_set, num_epochs, lr_range, weight_decay_range, conv2_filters_range, loss_weight_range, use_cuda = True, trials = 10, k = 5):\n",
        "  \"\"\"\n",
        "  Function that performs adaptive hyperparameter search for RNAUnet using CuPy (if cuda available)\n",
        "\n",
        "  Args:\n",
        "  - train_set: Pytorch training data set\n",
        "  - val_set: Pytorch validation data set\n",
        "  - num_epochs: Maximum number of epochs to train the model\n",
        "  - lr_range: Range og learning rate to search\n",
        "  - weight_decay_range: Range of weight decay values to search\n",
        "  - conv2_filters_range: Range of numbers of filters for the first hidden layer to search\n",
        "  - loss_weight_range: Range og values for weight of classification loss to search\n",
        "  - use_cuda: Boolean value indicating whether to use cuda if available\n",
        "  - trials: Number of trials to perform\n",
        "  - k: Number of folds for cross-validation\n",
        "\n",
        "  Returns:\n",
        "  - best_params: Dictionary containing the est hyperparameters found.\n",
        "  \"\"\"\n",
        "\n",
        "  if use_cuda and torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "  \n",
        "  def Kfold_cv(parameters: dict, k=5):\n",
        "    val_losses = 0.0\n",
        "    \n",
        "    #Split data into k folds: \n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42) \n",
        "\n",
        "    #Loop over folds\n",
        "    for train_idx, val_idx in kf:\n",
        "        #Split data intro training and validation sets\n",
        "        fold_train_set = torch.utils.data.Subset(train_set, train_idx)\n",
        "        fold_val_set = torch.utils.data.Subset(train_set, val_idx)\n",
        "\n",
        "        #Define data loaders\n",
        "        train_fold_loader = Dataloader(fold_train_set, batch_size=1, shuffle=True)\n",
        "        val_fold_loader = Dataloader(fold_val_set, batch_size=1)\n",
        "\n",
        "        #Define model\n",
        "        model = utils.RNAUnet_multi(channels = parameters[\"conv2_filters\"])\n",
        "        model.to(device)\n",
        "        optimizer = utils.adam_optimizer(model, parameters[\"lr\"], parameters[\"weight_decay\"])\n",
        "\n",
        "        #Train model\n",
        "        for epoch in num_epochs:\n",
        "            for input, output, label in train_fold_loader:\n",
        "                input, output, label = input.to(device), output.to(device), label.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                predicted, family = model(input, output)\n",
        "                loss = (1-parameters[\"loss_weight\"])*utils.dice_loss(predicted, output) + parameters[\"loss_weight\"]*F.cross_entropy(family, label)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            \n",
        "            #Evaluate model on validation set\n",
        "            val_loss = 0.0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for input, output, label in val_fold_loader: \n",
        "                    input, output, label = input.to(device), output.to(device), label.to(device)\n",
        "                    predicted, family = model(input)\n",
        "                    val_loss += ((1-parameters[\"loss_weight\"])*utils.dice_loss(predicted, output) + parameters[\"loss_weight\"]*F.cross_entropy(family, label)).item()\n",
        "            val_loss = val_loss/len(val_fold_loader)\n",
        "\n",
        "\n",
        "        val_losses += val_loss\n",
        "\n",
        "    return val_losses/k\n",
        "\n",
        "  best_loss = float('inf')\n",
        "  best_params ={}\n",
        "\n",
        "  #Define search space\n",
        "  params = {\n",
        "        \"lr\": lr_range,\n",
        "        \"weight_decay\": weight_decay_range,\n",
        "        \"conv2_filters\": conv2_filters_range,\n",
        "        \"loss_weight\": loss_weight_range,\n",
        "    }\n",
        "  \n",
        "\n",
        "  for i in range(trials):\n",
        "    #Get search space for this iteration\n",
        "    parameters = {'lr': cp.random.choice(params[\"lr\"]),\n",
        "                  'weight_decay': cp.random.choice(params[\"weight_decay\"]),\n",
        "                  'conv2_filters': cp.random.choice(params[\"conv2_filters\"]),\n",
        "                  'loss_weight': cp.random.choice(params[\"loss_weight\"])}\n",
        "    \n",
        "    val_loss = Kfold_cv(parameters, k)\n",
        "\n",
        "\n",
        "    #Update best hyperparameters if applicable\n",
        "    if val_loss < best_loss:\n",
        "      best_loss = val_loss\n",
        "      best_params = parameters\n",
        "      print(\"New best hyperparameters found: \") \n",
        "      print(best_params)\n",
        "      \n",
        "    #Update search space based on performance\n",
        "    if i > 0:\n",
        "      if val_loss < prev_val_loss:\n",
        "        if parameters[\"lr\"] in params[\"lr\"]:\n",
        "          params[\"lr\"].remove(parameters[\"lr\"])\n",
        "        if parameters[\"weight_decay\"] in params[\"weight_decay\"]:  \n",
        "          params[\"weight_decay\"].remove(parameters[\"weight_decay\"])\n",
        "        if parameters[\"conv2_filters\"] in params[\"conv2_filters\"]:\n",
        "          params[\"conv2_filters\"].remove(parameters[\"conv2_filters\"])\n",
        "        if parameters[\"loss_weight\"] in params[\"loss_weight\"]:\n",
        "          params[\"loss_weight\"].remove(parameters[\"loss_weight\"])\n",
        "          \n",
        "        prev_val_loss = val_loss\n",
        "\n",
        "    print(f\"Trial {i+1} completed. Best hyperparameters found: {best_params} with loss {best_loss}\")\n",
        "\n",
        "  return best_params\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA-zsKMWriiX"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zv0Vur_voeMU"
      },
      "outputs": [],
      "source": [
        "RNA_data = namedtuple('RNA_data', 'input output length family name pairs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WNg06ZQB3ol"
      },
      "outputs": [],
      "source": [
        "# Define the path to the zipped folder in your Google Drive\n",
        "tar_file_path = '/content/drive/MyDrive/data/experiment8.tar.gz'\n",
        "\n",
        "\n",
        "# Extract the tar.gz archive\n",
        "with tarfile.open(tar_file_path, 'r:gz') as tar:\n",
        "    tar.extractall('/content')\n",
        "\n",
        "file_list = [os.path.join('data', 'experiment8', file) for file in os.listdir('data/experiment8')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxvGx-pdlCCp"
      },
      "outputs": [],
      "source": [
        "train = pickle.load(open('/content/drive/MyDrive/data/experiment_train.pkl', 'rb'))\n",
        "valid = pickle.load(open('/content/drive/MyDrive/data/experiment_valid.pkl', 'rb'))\n",
        "\n",
        "family_map = pickle.load(open('/content/drive/MyDrive/data/experiment_familymap.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abCg6CrjpzB8"
      },
      "outputs": [],
      "source": [
        "# Define your train_dataset and validation_dataset\n",
        "train_dataset = utils.ImageToImageDataset(train, family_map)\n",
        "validation_dataset = utils.ImageToImageDataset(valid, family_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJyYjrt9rix7"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params = {\n",
        "        \"lr\": [0.01, 0.005, 0.001],\n",
        "        \"weight_decay\": [0.01, 0.001, 0.0001, 0],\n",
        "        \"conv2_filters\": [32, 64],\n",
        "        \"loss_weight\": [0.25, 0.5, 0.75],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "adaptive_hyperparameter_search(train_dataset, 10, params[\"lr\"], params[\"weight_decay\"], params[\"conv2_filters\"], params[\"loss_weight\"], use_cuda = True, trials = 10, k = 5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
